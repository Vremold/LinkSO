  You can think of a single-threaded program as one lonely entity moving around through your problem space and doing one thing at a time. Because there’s only one entity, you never have to think about the problem of two entities trying to use the same resource at the same time: problems such as two people trying to park in the same space, walk through a door at the same time, or even talk at the same time. 
  With concurrency, things aren’t lonely anymore, but you now have the possibility of two or more tasks interfering with each other. If you don’t prevent such a collision, you’ll have two    tasks trying to access the same bank account at the same time, print to the same printer, adjust the same valve, and so on. 
Improperly accessing resources
  Consider the following example, where one task generates even numbers and other tasks consume those numbers. Here, the only job of the consumer tasks is to check the validity of the even numbers. 
  First we’ll define EvenChecker, the consumer task, since it will be reused in all the subsequent examples. To decouple EvenChecker from the various types of generators that we will experiment with, we’ll create an abstract class called IntGenerator, which contains the minimum necessary methods that EvenChecker must know about: that it has a next( ) method and that it can be canceled. This class doesn’t implement the Generator interface, because it must produce an int, and generics don’t support primitive parameters. 
  IntGenerator has a cancel( ) method to change the state of a boolean canceled flag and isCanceled( ) to see whether the object has been canceled. Because the canceled flag is a boolean, it is atomic, which means that simple operations like assignment and value return happen without the possibility of interruption, so you can’t see the field in an intermediate state in the midst of those simple operations. The canceled flag is also volatile in order to ensure visibility. You’ll learn about atomicity and visibility later in this chapter. 
  Any IntGenerator can be tested with the following EvenChecker class: 
  Note that in this example the class that can be canceled is not Runnable. Instead, all the EvenChecker tasks that depend on the IntGenerator object test it to see whether it’s been canceled, as you can see in run( ). This way, the tasks that share the common resource (the IntGenerator) watch that resource for the signal to terminate. This eliminates the so-called race condition, where two or more tasks race to respond to a condition and thus collide or otherwise produce inconsistent results. You must be careful to think about and protect against all the possible ways a concurrent system can fail. For example, a task cannot depend on another task, because task shutdown order is not guaranteed. Here, by making tasks depend on a nontask object, we eliminate the potential race condition. 
  The test( ) method sets up and performs a test of any type of IntGenerator by starting a number of EvenCheckers that use the same IntGenerator. If the IntGenerator causes a failure, test( ) will report it and return; otherwise, you must press Control-C to terminate it. 
  EvenChecker tasks constantly read and test the values from their associated IntGenerator. Note that if generator.isCanceled( ) is true, run( ) returns, which tells the Executor in EvenChecker.test( ) that the task is complete. Any EvenChecker task can call cancel( ) on its associated IntGenerator, which will cause all other EvenCheckers using that IntGenerator to gracefully shut down. In later sections, you’ll see that Java contains more general mechanisms for termination of threads. 
  The first IntGenerator we’ll look at has a next( ) that produces a series of even values: 
  It’s possible for one task to call next( ) after another task has performed the first increment of currentEvenValue but not the second (at the place in the code commented "Danger point here!"). This puts the value into an "incorrect" state. To prove that this can happen, EvenChecker.test( ) creates a group of EvenChecker objects to continually read the output of an EvenGenerator and test to see if each one is even. If not, the error is reported and the program is shut down. 
  This program will eventually fail because the EvenChecker tasks are able to access the information in EvenGenerator while it’s in an "incorrect" state. However, it may not detect    the problem until the EvenGenerator has completed many cycles, depending on the particulars of your operating system and other implementation details. If you want to see it fail much faster, try putting a call to yield( ) between the first and second increments. This is part of the problem with multithreaded programs—they can appear to be correct even when there’s a bug, if the probability for failure is very low. 
  It’s important to note that the increment operation itself requires multiple steps, and the task can be suspended by the threading mechanism in the midst of an increment—that is, increment is not an atomic operation in Java. So even a single increment isn’t safe to do without protecting the task. 
Resolving shared resource contention
  The previous example shows a fundamental problem when you are using threads: You never know when a thread might be run. Imagine sitting at a table with a fork, about to spear the last piece of food on a platter, and as your fork reaches for it, the food suddenly vanishes— because your thread was suspended and another diner came in and ate the food. That’s the problem you’re dealing with when writing concurrent programs. For concurrency to work, you need some way to prevent two tasks from accessing the same resource, at least during critical periods. 
  Preventing this kind of collision is simply a matter of putting a lock on a resource when one task is using it. The first task that accesses a resource must lock it, and then the other tasks cannot access that resource until it is unlocked, at which time another task locks and uses it, and so on. If the front seat of the car is the limited resource, the child who shouts "shotgun!" acquires the lock (for the duration of that trip). 
  To solve the problem of thread collision, virtually all concurrency schemes serialize access to shared resources. This means that only one task at a time is allowed to access the shared resource. This is ordinarily accomplished by putting a clause around a piece of code that only allows one task at a time to pass through that piece of code. Because this clause produces mutual exclusion, a common name for such a mechanism is mutex. 
  Consider the bathroom in your house; multiple people (tasks driven by threads) may each want to have exclusive use of the bathroom (the shared resource). To access the bathroom, a person knocks on the door to see if it’s available. If so, they enter and lock the door. Any other task that wants to use the bathroom is "blocked" from using it, so those tasks wait at the door until the bathroom is available. 
  The analogy breaks down a bit when the bathroom is released and it comes time to give access to another task. There isn’t actually a line of people, and we don’t know for sure who gets the bathroom next, because the thread scheduler isn’t deterministic that way. Instead, it’s as if there is a group of blocked tasks milling about in front of the bathroom, and when the task that has locked the bathroom unlocks it and emerges, the one that happens to be nearest the door at the moment goes in. As noted earlier, suggestions can be made to the thread scheduler via yield( ) and setPriority( ), but these suggestions may not have much of an effect, depending on your platform and JVM implementation. 
  To prevent collisions over resources, Java has built-in support in the form of the synchronized keyword. When a task wishes to execute a piece of code guarded by the synchronized keyword, it checks to see if the lock is available, then acquires it, executes the code, and releases it. 
  The shared resource is typically just a piece of memory in the form of an object, but may also be a file, an I/O port, or something like a printer. To control access to a shared resource, you first put it inside an object. Then any method that uses the resource can be made synchronized. If a task is in a call to one of the synchronized methods, all other tasks are    blocked from entering any of the synchronized methods of that object until the first task returns from its call. 
  In production code, you’ve already seen that you should make the data elements of a class private and access that memory only through methods. You can prevent collisions by declaring those methods synchronized, like this: 
  synchronized void f() { /* ... */ } synchronized void g() { /* ... */ } 
  All objects automatically contain a single lock (also referred to as a monitor). When you call any synchronized method, that object is locked and no other synchronized method of that object can be called until the first one finishes and releases the lock. For the preceding methods, if f( ) is called for an object by one task, a different task cannot call f( ) or g( ) for the same object until f( ) is completed and releases the lock. Thus, there is a single lock that is shared by all the synchronized methods of a particular object, and this lock can be used to prevent object memory from being written by more than one task at a time. 
  Note that it’s especially important to make fields private when working with concurrency; otherwise the synchronized keyword cannot prevent another task from accessing a field directly, and thus producing collisions. 
  One task may acquire an object’s lock multiple times. This happens if one method calls a second method on the same object, which in turn calls another method on the same object, etc. The JVM keeps track of the number of times the object has been locked. If the object is unlocked, it has a count of zero. As a task acquires the lock for the first time, the count goes to one. Each time the same task acquires another lock on the same object, the count is incremented. Naturally, multiple lock acquisition is only allowed for the task that acquired the lock in the first place. Each time the task leaves a synchronized method, the count is decremented, until the count goes to zero, releasing the lock entirely for use by other tasks. 
  There’s also a single lock per class (as part of the Class object for the class), so that synchronized static methods can lock each other out from simultaneous access of static data on a class-wide basis. 
  10 When should you synchronize? Apply Brian’s Rule of Synchronization: 
  If you are writing a variable that might next be read by another thread, or reading a variable that might have last been written by another thread, you must use synchronization, and further, both the reader and the writer must synchronize using the same monitor lock. 
  If you have more than one method in your class that deals with the critical data, you must synchronize all relevant methods. If you synchronize only one of the methods, then the others are free to ignore the object lock and can be called with impunity. This is an important point: Every method that accesses a critical shared resource must be synchronized or it won’t work right. 
  Synchronizing the EvenGenerator  By adding synchronized to EvenGenerator.java, we can prevent the undesirable thread access: 
  A call to Thread.yield( ) is inserted between the two increments, to raise the likelihood of a context switch while currentEvenValue is in an odd state. Because the mutex prevents more than one task at a time in the critical section, this will not produce a failure, but calling yield( ) is a helpful way to promote a failure if it’s going to happen. 
  The first task that enters next( ) acquires the lock, and any further tasks that try to acquire the lock are blocked from doing so until the first task releases the lock. At that point, the scheduling mechanism selects another task that is waiting on the lock. This way, only one task at a time can pass through the code that is guarded by the mutex. 
  Exercise 11: (3) Create a class containing two data fields, and a method that manipulates those fields in a multistep process so that, during the execution of that method, those fields are in an "improper state" (according to some definition that you establish). Add methods to read the fields, and create multiple threads to call the various methods and show that the data is visible in its "improper state." Fix the problem using the synchronized keyword. 
  Using explicit Lock objects  The Java SE5 java.util.concurrent library also contains an explicit mutex mechanism defined in java.util.concurrent.locks. The Lock object must be explicitly created, locked and unlocked; thus, it produces less elegant code than the built-in form. However, it is more flexible for solving certain types of problems. Here is SynchronizedEvenGenerator.java rewritten to use explicit Locks: 
  MutexEvenGenerator adds a mutex called lock and uses the lock( ) and unlock( ) methods to create a critical section within next( ). When you are using Lock objects, it is important to internalize the idiom shown here: Right after the call to lock( ), you must place a try-finally statement with unlock( ) in the finally clause—this is the only way to guarantee that the lock is always released. Note that the return statement must occur inside the try clause to ensure that the unlock( ) doesn’t happen too early and expose the data to a second task. 
  Although the try-finally requires more code than using the synchronized keyword, it also represents one of the advantages of explicit Lock objects. If something fails using the synchronized keyword, an exception is thrown, but you don’t get the chance to do any cleanup in order to maintain your system in a good state. With explicit Lock objects, you can maintain proper state in your system using the finally clause. 
  In general, when you are using synchronized, there is less code to write, and the opportunity for user error is greatly reduced, so you’ll usually only use the explicit Lock objects when you’re solving special problems. For example, with the synchronized keyword, you can’t try and fail to acquire a lock, or try to acquire a lock for a certain amount of time and then give up—to do this, you must use the concurrent library: 
  A ReentrantLock allows you to try and fail to acquire the lock, so that if someone else already has the lock, you can decide to go off and do something else rather than waiting until it is free, as you can see in the untimed( ) method. In timed( ), an attempt is made to acquire the lock which can fail after 2 seconds (note the use of the Java SE5 TimeUnit class to specify units). In main( ), a separate Thread is created as an anonymous class, and it acquires the lock so that the untimed( ) and timed( ) methods have something to contend with. 
  The explicit Lock object also gives you finer-grained control over locking and unlocking than does the built-in synchronized lock. This is useful for implementing specialized synchronization structures, such as hand-overhand locking (also called lock coupling), used for traversing the nodes of a linked list—the traversal code must capture the lock of the next node before it releases the current node’s lock. 
Atomicity and volatility
  An incorrect piece of lore that is often repeated in Java threading discussions is, "Atomic operations do not need to be synchronized." An atomic operation is one that cannot be interrupted by the thread scheduler; if the operation begins, then it will run to completion before the possibility of a context switch. Relying on atomicity is tricky and dangerous—you should only try to use atomicity instead of synchronization if you are a concurrency expert, or you have help from such an expert. If you think you’re smart enough to play with this kind of fire, take this test: 
  The Goetz Test11: If you can write a high-performance JVM for a modern microprocessor, 12 then you are qualified to think about whether you can avoid synchronizing. 
  It’s useful to know about atomicity, and to know that, along with other advanced techniques, it was used to implement some of the more clever java.util.concurrent library components. But strongly resist the urge to rely on it yourself; see Brian’s Rule of Synchronization, presented earlier. 
  tongue-in-cheek comments from him. making important decisions about your project. If that person already is, then you’ve got trouble." 
  Atomicity applies to "simple operations" on primitive types except for longs and doubles. Reading and writing primitive variables other than long and double is guaranteed to go to and from memory as indivisible (atomic) operations. However, the JVM is allowed to perform reads and writes of 64- bit quantities (long and double variables) as two separate read or write, and then different tasks could see incorrect results (this is sometimes called word tearing, because you might see the value after only part of it has been changed). However, you do get atomicity (for simple assignments and returns) if you use the volatile keyword when defining a long or double variable (note that volatile was not working properly before Java SE5). Different JVMs are free to provide stronger guarantees, but you should not rely on platform-specific features. 
  Atomic operations are thus not interruptible by the threading mechanism. Expert programmers can take advantage of this to write lock-free code, which does not need to be synchronized. But even this is an oversimplification. Sometimes, even when it seems like an atomic operation should be safe, it may not be. Readers of this book will typically not be able to pass the aforementioned Goetz Test, and will thus not be qualified to try to replace synchronization with atomic operations. Trying to remove synchronization is usually a sign of premature optimization, and will cause you a lot of trouble, probably without gaining much, or anything. 
  On multiprocessor systems (which are now appearing in the form of multicore processors— multiple CPUs on a single chip), visibility rather than atomicity is much more of an issue than on single-processor systems. Changes made by one task, even if they’re atomic in the sense of not being interruptible, might not be visible to other tasks (the changes might be temporarily stored in a local processor cache, for example), so different tasks will have a different view of the application’s state. The synchronization mechanism, on the other hand, forces changes by one task on a multiprocessor system to be visible across the application. Without synchronization, it’s indeterminate when changes become visible. 
  The volatile keyword also ensures visibility across the application. If you declare a field to be volatile, this means that as soon as a write occurs for that field, all reads will see the change. This is true even if local caches are involved—volatile fields are immediately written through to main memory, and reads occur from main memory. 
  It’s important to understand that atomicity and volatility are distinct concepts. An atomic operation on a non-volatile field will not necessarily be flushed to main memory, and so another task that reads that field will not necessarily see the new value. If multiple tasks are accessing a field, that field should be volatile; otherwise, the field should only be accessed via synchronization. Synchronization also causes flushing to main memory, so if a field is completely guarded by synchronized methods or blocks, it is not necessary to make it volatile. 
  Any writes that a task makes will be visible to that task, so you don’t need to make a field volatile if it is only seen within a task. 
  volatile doesn’t work when the value of a field depends on its previous value (such as incrementing a counter), nor does it work on fields whose values are constrained by the values of other fields, such as the lower and upper bound of a Range class which must obey the constraint lower <= upper. 
  It’s typically only safe to use volatile instead of synchronized if the class has only one mutable field. Again, your first choice should be to use the synchronized keyword—that’s the safest approach, and trying to do anything else is risky. 
  What qualifies as an atomic operation? Assignment and returning the value in a field will usually be atomic. However, in C++ even the following might be atomic: 
  i++; // Might be atomic in C++ i +=2; // Might be atomic in C++  But in C++, this depends on the compiler and processor. You’re unable to write cross- platform code in C++ that relies on atomicity, because C++ doesn’t have a consistent 13 memory model, as Java does (in Java SEs). 
  In Java, the above operations are definitely not atomic, as you can see from the JVM instructions produced by the following methods: 
  Each instruction produces a "get" and a "put," with instructions in between. So in between getting and putting, another task could modify the field, and thus the operations are not atomic. 
  If you blindly apply the idea of atomicity, you see that getValue( ) in the following program fits the description: 
  However, the program will find non-even values and terminate. Although return i is indeed an atomic operation, the lack of synchronization allows the value to be read while the object is in an unstable intermediate state. On top of this, since i is also not volatile, there will be visibility problems. Both getValue( ) and evenIncrement( ) must be synchronized. Only concurrency experts are qualified to attempt optimizations in situations like this; again, you should apply Brian’s Rule of Synchronization. 
  As a second example, consider something even simpler: a class that produces serial numbers.14 Each time nextSerialNumber( ) is called, it must return a unique value to the caller: 
  SerialNumberGenerator is about as simple a class as you can imagine, and if you’re coming from C++ or some other low-level background, you might expect the increment to be an atomic operation, because a C++ increment can often be implemented as a microprocessor instruction (although not in any reliable, cross-platform fashion). As noted before, however, a Java increment is not atomic and involves both a read and a write, so there’s room for threading problems even in such a simple operation. As you shall see, volatility isn’t actually the issue here; the real problem is that nextSerialNumber( ) accesses a shared, mutable value without synchronizing. 
  The serialNumber field is volatile because it is possible for each thread to have a local stack and maintain copies of some variables there. If you define a variable as volatile, it tells the compiler not to do any optimizations that would remove reads and writes that keep the field in exact synchronization with the local data in the threads. In effect, reads and writes go directly to memory, and are not cached, volatile also restricts compiler reordering of accesses during optimization. However, volatile doesn’t affect the fact that an increment isn’t an atomic operation. 
  Basically, you should make a field volatile if that field could be simultaneously accessed by multiple tasks, and at least one of those accesses is a write. For example, a field that is used as a flag to stop a task must be declared volatile; otherwise, that flag could be cached in a       register, and when you make changes to the flag from outside the task, the cached value wouldn’t be changed and the task wouldn’t know it should stop. 
  To test SerialNumberGenerator, we need a set that doesn’t run out of memory, in case it takes a long time to detect a problem. The CircularSet shown here reuses the memory used to store ints, with the assumption that by the time you wrap around, the possibility of a collision with the overwritten values is minimal. The add( ) and contains( ) methods are synchronized to prevent thread collisions: 
  SerialNumberChecker contains a static CircularSet that holds all the serial numbers that have been produced, and a nested SerialChecker class that ensures the serial numbers are unique. By creating multiple tasks to contend over serial numbers, you’ll discover that the tasks eventually get a duplicate serial number, if you let it run long enough. To solve the problem, add the synchronized keyword to nextSerialNumber( ). 
  The atomic operations that are supposed to be safe are the reading and assignment of primitives. However, as seen in AtomicityTest.java, it’s still easily possible to use an atomic operation that accesses your object while it’s in an unstable intermediate state. Making assumptions about this issue is tricky and dangerous. The most sensible thing to do is just to follow Brian’s Rule of Synchronization. 
  Exercise 12: (3) Repair AtomicityTest.java using the synchronized keyword. Can you demonstrate that it is now correct? 
  Exercise 13: (1) Repair SerialNumberChecker.java using the synchronized keyword. Can you demonstrate that it is now correct? 
Atomic classes
  Java SE5 introduces special atomic variable classes such as Atomiclnteger, AtomicLong, AtomicReference, etc. that provide an atomic conditional update operation of the form: 
  boolean compareAndSet(expectedValue, updateValue); 
  These are for fine-tuning to use machine-level atomicity that is available on some modern processors, so you generally don’t need to worry about using them. Occasionally they come in handy for regular coding, but again when performance tuning is involved. For example, we can rewrite AtomicityTest.java to use Atomiclnteger: 
  Here we’ve eliminated the synchronized keyword by using AtomicInteger instead. Because the program doesn’t fail, a Timer is added to automatically abort after 5 seconds. 
  Here is MutexEvenGenerator.java rewritten to use Atomiclnteger: 
  Again, all other forms of synchronization have been eliminated by using AtomicInteger. 
  It should be emphasized that the Atomic classes were designed to build the classes in java.util.concurrent, and that you should use them in your own code only under special circumstances, and even then only when you can ensure that there are no other possible problems. It’s generally safer to rely on locks (either the synchronized keyword or explicit Lock objects). 
  Exercise 14: (4) Demonstrate that java.util.Timer scales to large numbers by creating a program that generates many Timer objects that perform some simple task when the timeout completes. 
Critical sections
  Sometimes, you only want to prevent multiple thread access to part of the code inside a method instead of the entire method. The section of code you want to isolate this way is called a critical section and is created using the synchronized keyword. Here, synchronized is used to specify the object whose lock is being used to synchronize the enclosed code: 
  synchronized(syncObject) { // This code can be accessed    // by only one task at a time } 
  This is also called a synchronized block; before it can be entered, the lock must be acquired on syncObject. If some other task already has this lock, then the critical section cannot be entered until the lock is released. The following example compares both synchronization approaches by showing how the time available for other tasks to access an object is significantly increased by using a synchronized block instead of synchronizing an entire method. In addition, it shows how an unprotected class can be used in a multithreaded situation if it is controlled and protected by another class: 
  As noted, Pair is not thread-safe because its invariant (admittedly arbitrary) requires that both variables maintain the same values. In addition, as seen earlier in this chapter, the increment operations are not thread-safe, and because none of the methods are synchronized, you can’t trust a Pair object to stay uncorrupted in a threaded program. 
  You can imagine that someone hands you the non-thread-safe Pair class, and you need to use it in a threaded environment. You do this by creating the PairManager class, which holds a Pair object and controls all access to it. Note that the only public methods are getPair( ), which is synchronized, and the abstract increment( ). Synchronization for increment( ) will be handled when it is implemented. 
  The structure of PairManager, where functionality implemented in the base class uses one or more abstract methods defined in derived classes, is called a Template Method in Design Patterns parlance.15 Design patterns allow you to encapsulate change in your code; here, the part that is changing is the method increment( ). In PairManager1 the entire increment( ) method is synchronized, but in PairManager2 only part of increment( ) is synchronized by using a synchronized block. Note that the synchronized keyword is not part of the method signature and thus may be added during overriding. 
  The store( ) method adds a Pair object to a synchronized ArrayList, so this operation is thread safe. Thus, it doesn’t need to be guarded, and is placed outside of the synchronized block in PairManager2. 
  PairManipulator is created to test the two different types of PairManagers by calling increment( ) in a task while a PairChecker is run from another task. To trace how often it is able to run the test, PairChecker increments checkCounter every time it is successful. In main( ), two PairManipulator objects are created and allowed to run for a while, after which the results of each PairManipulator are shown. 
  Although you will probably see a lot of variation in output from one run to the next, in general you will see that PairManager1.increment( ) does not allow the PairChecker       nearly as much access as PairManager2.increment( ), which has the synchronized block and thus provides more unlocked time. This is typically the reason to use a synchronized block instead of synchronizing the whole method: to allow other tasks more access (as long as it is safe to do so). 
  You can also use explicit Lock objects to create critical sections: 
  This reuses most of CriticalSection.java and creates new PairManager types that use explicit Lock objects. ExplicitPairManager2 shows the creation of a critical section using a Lock object; the call to store( ) is outside of the critical section. 
Synchronizing on other objects
  A synchronized block must be given an object to synchronize upon, and usually the most sensible object to use is just the current object that the method is being called for: synchronized(this), which is the approach taken in PairManager2. That way, when the lock is acquired for the synchronized block, other synchronized methods and critical sections in the object cannot be called. So the effect of the critical section, when synchronizing on this, is simply to reduce the scope of synchronization. 
  Sometimes you must synchronize on another object, but if you do this you must ensure that all relevant tasks are synchronizing on the same object. The following example demonstrates that two tasks can enter an object when the methods in that object synchronize on different locks: 
  DualSync.f( ) synchronizes on this (by synchronizing the entire method), and g( ) has a synchronized block that synchronizes on syncObject. Thus, the two synchronizations are independent. This is demonstrated in main( ) by creating a Thread that calls f( ). The main( ) thread is used to call g( ). You can see from the output that both methods are running at the same time, so neither one is blocked by the synchronization of the other. 
  Exercise 15: (1) Create a class with three methods containing critical sections that all synchronize on the same object. Create multiple tasks to demonstrate that only one of these methods can run at a time. Now modify the methods so that each one synchronizes on a different object and show that all three methods can be running at once. 
  Exercise 16: (1) Modify Exercise 15 to use explicit Lock objects. 
Thread local storage
  A second way to prevent tasks from colliding over shared resources is to eliminate the sharing of variables. Thread local storage is a mechanism that automatically creates different storage for the same variable, for each different thread that uses an object. Thus, if you have five threads using an object with a variable x, thread local storage generates five different pieces of storage for x. Basically, they allow you to associate state with a thread. 
  The creation and management of thread local storage is taken care of by the java.lang.ThreadLocal class, as seen here: 
  ThreadLocal objects are usually stored as static fields. When you create a ThreadLocal object, you are only able to access the contents of the object using the get( ) and set( ) methods. The get( ) method returns a copy of the object that is associated with that thread, and set( ) inserts its argument into the object stored for that thread, returning the old object that was in storage. The increment( ) and get( ) methods demonstrate this in ThreadLocalVariableHolder. Notice that increment( ) and get( ) are not synchronized, because ThreadLocal guarantees that no race condition can occur. 
  When you run this program, you’ll see evidence that the individual threads are each allocated their own storage, since each one keeps its own count even though there’s only one ThreadLocalVariableHolder object. 
